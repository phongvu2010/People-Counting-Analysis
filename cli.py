"""
Module ƒë·ªãnh nghƒ©a giao di·ªán d√≤ng l·ªánh (CLI) cho ·ª©ng d·ª•ng.

S·ª≠ d·ª•ng Typer ƒë·ªÉ t·∫°o c√°c c√¢u l·ªánh ti·ªán √≠ch, bao g·ªìm:
- `run-etl`: Ch·∫°y quy tr√¨nh ETL ƒëa lu·ªìng ƒë·ªÉ ƒë·ªìng b·ªô d·ªØ li·ªáu.
- `init-db`: Kh·ªüi t·∫°o c√°c ƒë·ªëi t∆∞·ª£ng c·∫ßn thi·∫øt trong DuckDB (v√≠ d·ª•: VIEWs).
- `serve`: Kh·ªüi ch·∫°y web server FastAPI.
"""

import contextlib
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from typing import Iterator
from typing_extensions import Annotated

import duckdb
import pandera.errors as pa_errors
import requests
import typer
import uvicorn
from duckdb import DuckDBPyConnection
from duckdb import Error as DuckdbError
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine
from sqlalchemy.exc import SQLAlchemyError
from tenacity import (
    before_sleep_log,
    retry,
    retry_if_exception,
    stop_after_attempt,
    wait_fixed,
)

from app.core.config import settings, TableConfig
from app.etl import extract, state, transform
from app.etl.load import ParquetLoader, prepare_destination, refresh_duckdb_table
from app.utils.logger import setup_logging

# C·∫•u h√¨nh logging ngay t·ª´ ƒë·∫ßu ƒë·ªÉ √°p d·ª•ng cho to√†n b·ªô ·ª©ng d·ª•ng.
setup_logging("configs/logger.yaml")
logger = logging.getLogger(__name__)

# Kh·ªüi t·∫°o Typer App ƒë·ªÉ qu·∫£n l√Ω c√°c c√¢u l·ªánh.
cli_app = typer.Typer(
    help="CLI ƒë·ªÉ qu·∫£n l√Ω v√† v·∫≠n h√†nh ·ª©ng d·ª•ng Analytics iCount People."
)


@contextlib.contextmanager
def _get_database_connections() -> Iterator[tuple[Engine, DuckDBPyConnection]]:
    """
    Context manager ƒë·ªÉ qu·∫£n l√Ω v√≤ng ƒë·ªùi k·∫øt n·ªëi ƒë·∫øn c√°c database.

    Yields:
        M·ªôt tuple ch·ª©a (SQLAlchemy Engine, DuckDB Connection).

    Raises:
        SQLAlchemyError: N·∫øu kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi MS SQL Server.
        DuckdbError: N·∫øu kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi DuckDB.
    """
    sql_engine, duckdb_conn = None, None
    try:
        logger.info("ƒêang thi·∫øt l·∫≠p k·∫øt n·ªëi t·ªõi MS SQL Server...")
        sql_engine = create_engine(
            settings.db.sqlalchemy_db_uri, pool_pre_ping=True
        )
        with sql_engine.connect() as conn:
            conn.execute(text("SELECT 1"))  # Ping ƒë·ªÉ ki·ªÉm tra
        logger.info("‚úÖ K·∫øt n·ªëi MS SQL Server th√†nh c√¥ng.")

        logger.info("ƒêang thi·∫øt l·∫≠p k·∫øt n·ªëi t·ªõi DuckDB...")
        duckdb_path = str(settings.DUCKDB_PATH.resolve())
        duckdb_conn = duckdb.connect(database=duckdb_path, read_only=False)
        logger.info(f"‚úÖ K·∫øt n·ªëi DuckDB ('{duckdb_path}') th√†nh c√¥ng.\n")

        yield sql_engine, duckdb_conn

    except SQLAlchemyError as e:
        logger.critical(f"‚ùå L·ªói nghi√™m tr·ªçng khi k·∫øt n·ªëi SQL Server: {e}", exc_info=True)
        raise
    except DuckdbError as e:
        logger.critical(f"‚ùå L·ªói nghi√™m tr·ªçng khi k·∫øt n·ªëi DuckDB: {e}", exc_info=True)
        raise
    finally:
        if sql_engine:
            sql_engine.dispose()
            logger.debug("K·∫øt n·ªëi SQL Server ƒë√£ ƒë∆∞·ª£c ƒë√≥ng.")
        if duckdb_conn:
            duckdb_conn.close()
            logger.debug("K·∫øt n·ªëi DuckDB ƒë√£ ƒë∆∞·ª£c ƒë√≥ng.")


def _is_retryable_exception(exception: BaseException) -> bool:
    """Ki·ªÉm tra xem m·ªôt exception c√≥ thu·ªôc lo·∫°i c√≥ th·ªÉ th·ª≠ l·∫°i hay kh√¥ng."""
    return isinstance(exception, (SQLAlchemyError, DuckdbError, IOError))


@retry(
    stop=stop_after_attempt(3),
    wait=wait_fixed(15),
    before_sleep=before_sleep_log(logger, logging.WARNING),
    retry=retry_if_exception(_is_retryable_exception),
)
def _process_table(
    sql_engine: Engine,
    duckdb_conn: DuckDBPyConnection,
    config: TableConfig,
    etl_state: dict,
    state_lock: Lock,
):
    """
    X·ª≠ l√Ω to√†n b·ªô pipeline ETL cho m·ªôt b·∫£ng duy nh·∫•t (Extract -> Transform -> Load).

    H√†m n√†y ƒë∆∞·ª£c b·ªçc b·ªüi decorator @retry ƒë·ªÉ t·ª± ƒë·ªông th·ª≠ l·∫°i n·∫øu g·∫∑p l·ªói
    li√™n quan ƒë·∫øn k·∫øt n·ªëi ho·∫∑c I/O.
    """
    logger.info(
        f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω b·∫£ng: '{config.source_table}' -> '{config.dest_table}' "
        f"(Incremental: {config.incremental})"
    )
    prepare_destination(config)

    last_timestamp = state.get_last_timestamp(etl_state, config.dest_table)
    data_iterator = extract.from_sql_server(sql_engine, config, last_timestamp)

    total_rows, max_ts_in_run = 0, None

    try:
        with ParquetLoader(config) as loader:
            for chunk in data_iterator:
                transformed_chunk = transform.run_transformations(chunk, config)
                if transformed_chunk.empty:
                    continue

                loader.write_chunk(transformed_chunk)
                total_rows += len(transformed_chunk)

                current_max_ts = transform.get_max_timestamp(
                    transformed_chunk, config
                )
                if current_max_ts and (
                    max_ts_in_run is None or current_max_ts > max_ts_in_run
                ):
                    max_ts_in_run = current_max_ts

        if total_rows > 0:
            logger.info(
                f"ƒê√£ x·ª≠ l√Ω {total_rows:,} d√≤ng. B·∫Øt ƒë·∫ßu n·∫°p v√†o DuckDB..."
            )
            refresh_duckdb_table(duckdb_conn, config, loader.has_written_data)
            logger.info(f"N·∫°p d·ªØ li·ªáu v√†o DuckDB '{config.dest_table}' ho√†n t·∫•t.")

            if config.incremental and max_ts_in_run:
                with state_lock:
                    state.update_timestamp(
                        etl_state, config.dest_table, max_ts_in_run
                    )
                    state.save_etl_state(etl_state)
        else:
            logger.info(f"Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi cho b·∫£ng '{config.dest_table}'.")

        return config.dest_table

    except pa_errors.SchemaErrors as e:
        logger.error(
            f"‚ùå L·ªói validation d·ªØ li·ªáu cho '{config.dest_table}': {e}",
            exc_info=True,
        )
        raise
    except Exception as e:
        logger.error(
            f"‚ùå L·ªói kh√¥ng th·ªÉ ph·ª•c h·ªìi sau c√°c l·∫ßn th·ª≠ l·∫°i cho "
            f"'{config.dest_table}': {e}",
            exc_info=True,
        )
        raise


def _trigger_cache_clear(host: str, port: int):
    """G·ª≠i y√™u c·∫ßu POST ƒë·∫øn API server ƒë·ªÉ x√≥a cache."""
    if not settings.INTERNAL_API_TOKEN:
        logger.warning(
            "INTERNAL_API_TOKEN ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh, b·ªè qua vi·ªác x√≥a cache."
        )
        return

    clear_cache_url = f"http://{host}:{port}/api/v1/admin/clear-cache"
    headers = {"X-Internal-Token": settings.INTERNAL_API_TOKEN}

    try:
        logger.info(f"ƒêang g·ª≠i y√™u c·∫ßu x√≥a cache ƒë·∫øn {clear_cache_url}...")
        response = requests.post(clear_cache_url, headers=headers, timeout=10)
        response.raise_for_status()
        if response.status_code == 204:
            logger.info("‚úÖ Y√™u c·∫ßu x√≥a cache ƒë∆∞·ª£c API server ch·∫•p nh·∫≠n.")
    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå Kh√¥ng th·ªÉ x√≥a cache c·ªßa API server: {e}")
        logger.warning(
            "L∆∞u √Ω: D·ªØ li·ªáu m·ªõi c√≥ th·ªÉ m·∫•t t·ªõi 30 ph√∫t ƒë·ªÉ hi·ªÉn th·ªã tr√™n dashboard."
        )


@cli_app.command()
def run_etl(
    max_workers: int = typer.Option(
        4, help="S·ªë lu·ªìng t·ªëi ƒëa ƒë·ªÉ x·ª≠ l√Ω ETL song song."
    ),
    clear_cache: bool = typer.Option(
        True, help="T·ª± ƒë·ªông x√≥a cache c·ªßa API server sau khi ETL th√†nh c√¥ng."
    ),
    api_host: str = typer.Option(
        "127.0.0.1", help="Host c·ªßa API server ƒëang ch·∫°y."
    ),
    api_port: int = typer.Option(8000, help="Port c·ªßa API server ƒëang ch·∫°y."),
):
    """Ch·∫°y quy tr√¨nh ETL ƒëa lu·ªìng ƒë·ªÉ ƒë·ªìng b·ªô d·ªØ li·ªáu t·ª´ SQL Server sang DuckDB."""
    logger.info("=" * 60)
    logger.info(f"üöÄ B·∫ÆT ƒê·∫¶U QUY TR√åNH ETL (T·ªëi ƒëa {max_workers} lu·ªìng)")
    logger.info("=" * 60)

    succeeded, failed = [], []
    etl_state = state.load_etl_state()
    state_lock = Lock()

    tables_to_process = sorted(
        settings.TABLE_CONFIG.values(), key=lambda cfg: cfg.processing_order
    )
    total_tables = len(tables_to_process)

    try:
        with _get_database_connections() as (sql_engine, duckdb_conn):
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_table = {
                    executor.submit(
                        _process_table,
                        sql_engine,
                        duckdb_conn,
                        config,
                        etl_state,
                        state_lock,
                    ): config
                    for config in tables_to_process
                }

                for future in as_completed(future_to_table):
                    config = future_to_table[future]
                    try:
                        result = future.result()
                        succeeded.append(result)
                        logger.info(f"‚úÖ X·ª≠ l√Ω th√†nh c√¥ng '{config.dest_table}'.\n")
                    except Exception:
                        failed.append(config.dest_table)
                        logger.error(
                            f"‚ùå X·ª≠ l√Ω '{config.dest_table}' th·∫•t b·∫°i sau t·∫•t c·∫£ "
                            f"c√°c l·∫ßn th·ª≠ l·∫°i.\n"
                        )
    except Exception as e:
        logger.critical(
            f"Quy tr√¨nh ETL b·ªã d·ª´ng ƒë·ªôt ng·ªôt do l·ªói k·∫øt n·ªëi ban ƒë·∫ßu: {e}"
        )

    finally:
        if clear_cache and succeeded:
            _trigger_cache_clear(host=api_host, port=api_port)

        logger.info("=" * 60)
        logger.info("üìä T√ìM T·∫ÆT K·∫æT QU·∫¢ ETL")
        logger.info(f"T·ªïng s·ªë b·∫£ng: {total_tables}")
        logger.info(f"‚úÖ Th√†nh c√¥ng: {len(succeeded)}")
        logger.info(f"‚ùå Th·∫•t b·∫°i: {len(failed)}")
        if failed:
            logger.warning(f"Danh s√°ch b·∫£ng th·∫•t b·∫°i: {', '.join(failed)}")
        logger.info("=" * 60 + "\n")


@cli_app.command()
def init_db():
    """Kh·ªüi t·∫°o ho·∫∑c c·∫≠p nh·∫≠t c√°c VIEWs c·∫ßn thi·∫øt trong DuckDB."""
    logger.info("B·∫Øt ƒë·∫ßu kh·ªüi t·∫°o/c·∫≠p nh·∫≠t VIEW 'v_traffic_normalized'...")

    scale = settings.OUTLIER_SCALE_RATIO
    then_logic_in = (
        f"CAST(ROUND(a.visitors_in * {scale}, 0) AS INTEGER)" if scale > 0 else "1"
    )
    then_logic_out = (
        f"CAST(ROUND(a.visitors_out * {scale}, 0) AS INTEGER)" if scale > 0 else "1"
    )

    create_view_sql = f"""
    CREATE OR REPLACE VIEW v_traffic_normalized AS
    SELECT
        CAST(a.recorded_at AS TIMESTAMP) AS record_time,
        b.store_name,
        CASE
            WHEN a.visitors_in > {settings.OUTLIER_THRESHOLD} THEN {then_logic_in}
            ELSE a.visitors_in
        END AS in_count,
        CASE
            WHEN a.visitors_out > {settings.OUTLIER_THRESHOLD} THEN {then_logic_out}
            ELSE a.visitors_out
        END AS out_count,
        -- D·ªãch chuy·ªÉn th·ªùi gian ƒë·ªÉ ng√†y l√†m vi·ªác b·∫Øt ƒë·∫ßu t·ª´ 00:00
        (record_time - INTERVAL '{settings.WORKING_HOUR_START} hours') AS adjusted_time
    FROM fact_traffic AS a
    LEFT JOIN dim_stores AS b ON a.store_id = b.store_id;
    """

    try:
        db_path = str(settings.DUCKDB_PATH.resolve())
        with duckdb.connect(database=db_path, read_only=False) as conn:
            conn.execute(create_view_sql)
        logger.info("‚úÖ ƒê√£ t·∫°o/c·∫≠p nh·∫≠t th√†nh c√¥ng VIEW 'v_traffic_normalized'.")
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi kh·ªüi t·∫°o VIEW: {e}", exc_info=True)
        raise typer.Exit(code=1)


@cli_app.command()
def serve(
    host: Annotated[
        str, typer.Option(help="Host ƒë·ªÉ ch·∫°y server.")
    ] = "127.0.0.1",
    port: Annotated[int, typer.Option(help="Port ƒë·ªÉ ch·∫°y server.")] = 8000,
    reload: Annotated[
        bool, typer.Option(help="T·ª± ƒë·ªông t·∫£i l·∫°i khi code thay ƒë·ªïi.")
    ] = True,
):
    """Kh·ªüi ch·∫°y ·ª©ng d·ª•ng web FastAPI v·ªõi Uvicorn."""
    logger.info(f"üöÄ Kh·ªüi ch·∫°y FastAPI server t·∫°i http://{host}:{port}")
    uvicorn.run(
        "app.main:api_app",
        host=host,
        port=port,
        reload=reload,
        reload_dirs=["app", "configs", "template"],
    )


if __name__ == "__main__":
    cli_app()
